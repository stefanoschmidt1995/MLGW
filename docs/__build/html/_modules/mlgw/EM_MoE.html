

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>mlgw.EM_MoE &#8212; mlgw 3.0.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/mlgw/EM_MoE';</script>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    <p class="title logo__title">mlgw 3.0.0 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Usage</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../usage/install.html">Installing <code class="docutils literal notranslate"><span class="pre">mlgw</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../usage/overview.html">What <code class="docutils literal notranslate"><span class="pre">mlgw</span></code> can do for you</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../api_reference/GW_generator.html">Module GW_generator.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_reference/NN_model.html">Module NN_model.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_reference/EM_MoE.html">Module EM_MoE.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_reference/ML_routines.html">Module ML_routines.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_reference/GW_helper.html">Module GW_helper.py</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_reference/fit_model.html">Module fit_model.py</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../about.html">About</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <h1>Source code for mlgw.EM_MoE</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Module EM_MoE.py</span>
<span class="sd">================</span>
<span class="sd">	Definition of the following ML models useful for developing a full MoE model:</span>
<span class="sd">		Mixture of Experts model</span>
<span class="sd">			class MoE_model: implements a MoE model with methods for fitting and making predictions</span>
<span class="sd">		Softmax regression</span>
<span class="sd">			class softmax_regression: implements a softmax regression with methods for fitting and making predictions. This is the standard classifier used by MoE model.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1">#################</span>

<span class="kn">import</span> <span class="nn">scipy.stats</span>
<span class="kn">import</span> <span class="nn">scipy.optimize</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1">################# MoE_model class</span>
<div class="viewcode-block" id="MoE_model"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.MoE_model">[docs]</a><span class="k">class</span> <span class="nc">MoE_model</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
	<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">MoE_model</span>
<span class="sd">=========</span>
<span class="sd">	It represents and fits a MoE model of the form:</span>
<span class="sd">		p(y_i | x_i, params) = sum_{k=1}^K S(x_i)_k * N(y_i|&lt;w_k,x_i&gt;+b_k, sigma_k)</span>
<span class="sd">	where S(x_i) is any model. A default for S(x_i) is the softmax model S(x_i)= softmax(V*x_i).</span>
<span class="sd">	The model is fitted trough EM algorithm.</span>
<span class="sd">	&quot;&quot;&quot;</span>
	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">gating_function</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">		Initialize the model.</span>
<span class="sd">		The gating function must be specified through an object with methods:</span>
<span class="sd">			fit(x_train(N,D)		labels_train (N,K)) returning</span>
<span class="sd">			predict((x_test(N,D)))	returning labels_predicted (N,K)</span>
<span class="sd">			save					for saving to file the entire model</span>
<span class="sd">			load					to load from file the entire model</span>
<span class="sd">		Gating function must use cross entropy loss function.</span>
<span class="sd">		If None a default softmax regression model is used built from class softmax_regression.</span>
<span class="sd">		The gating function obkect must be already initialized properly.</span>
<span class="sd">		Input:</span>
<span class="sd">			D						dimensionality of input space</span>
<span class="sd">			K						number of experts for the model</span>
<span class="sd">			gating_function (obj)	an object which represents the gating function</span>
<span class="sd">			bias					whether to use a bias in the expert model</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">D</span> 
		<span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">K</span> 
		<span class="k">if</span> <span class="n">gating_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">gating</span> <span class="o">=</span> <span class="n">softmax_regression</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">K</span><span class="p">)</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">gating</span> <span class="o">=</span> <span class="n">gating_function</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">))</span> <span class="c1">#[w_1, ..., w_K]</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,))</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,))</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">initialized</span> <span class="o">=</span> <span class="kc">False</span>

<div class="viewcode-block" id="MoE_model.get_iperparams"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.MoE_model.get_iperparams">[docs]</a>	<span class="k">def</span> <span class="nf">get_iperparams</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	get_iperparams</span>
<span class="sd">	==============</span>
<span class="sd">		Returns values of D and K.</span>
<span class="sd">		Outpu:</span>
<span class="sd">			(D,K)	(dimensionality of input space, number of experts)</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoE_model.save"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.MoE_model.save">[docs]</a>	<span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exp_file</span><span class="p">,</span> <span class="n">gat_file</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	save</span>
<span class="sd">	====</span>
<span class="sd">		Saves the model to file.</span>
<span class="sd">		Input:</span>
<span class="sd">			exp_file	file to save the expert model to</span>
<span class="sd">			gat_file	file to save the model for gating function</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">to_save</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">))</span> <span class="c1">#(2, K)</span>
		<span class="n">to_save</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span><span class="n">to_save</span><span class="p">)</span> <span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="c1">#(D+2,K)</span>
		<span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="n">exp_file</span><span class="p">,</span> <span class="n">to_save</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">gating</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">gat_file</span><span class="p">)</span>
		<span class="k">return</span></div>
	
<div class="viewcode-block" id="MoE_model.load"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.MoE_model.load">[docs]</a>	<span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exp_file</span><span class="p">,</span> <span class="n">gat_file</span><span class="p">,</span> <span class="n">load_function</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	load</span>
<span class="sd">	====</span>
<span class="sd">		Load the model from file. It changes parameters D and K if required.</span>
<span class="sd">		Input:</span>
<span class="sd">			exp_file		file to load the expert model from</span>
<span class="sd">			gat_file		file to load the model for gating function from (using function load_function)</span>
<span class="sd">			gating_function	function for loading the gating function model from file</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="n">exp_file</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[:</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">2</span><span class="p">,:]</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">2</span><span class="p">,:]</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>
		<span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="o">==</span><span class="mi">0</span><span class="p">):</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">True</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
		
		<span class="k">if</span> <span class="n">load_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">temp_load</span> <span class="o">=</span> <span class="n">softmax_regression</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
			<span class="n">load_function</span> <span class="o">=</span> <span class="n">temp_load</span><span class="o">.</span><span class="n">load</span>

		<span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">gating</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">gating</span> <span class="o">=</span> <span class="n">load_function</span><span class="p">(</span><span class="n">gat_file</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">initialized</span> <span class="o">=</span>  <span class="kc">True</span>
		<span class="k">return</span></div>

<div class="viewcode-block" id="MoE_model.experts_predictions"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.MoE_model.experts_predictions">[docs]</a>	<span class="k">def</span> <span class="nf">experts_predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	experts_predictions</span>
<span class="sd">	===================</span>
<span class="sd">		Returns the predictions of the experts.</span>
<span class="sd">		Input:</span>
<span class="sd">			X_test (N,D)	test points</span>
<span class="sd">		Output:</span>
<span class="sd">			y_test (N,K)	experts predictions</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span><span class="mi">1</span><span class="p">:</span>
			<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
		<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span></div>

<div class="viewcode-block" id="MoE_model.get_gating_probs"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.MoE_model.get_gating_probs">[docs]</a>	<span class="k">def</span> <span class="nf">get_gating_probs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	get_gating_probs</span>
<span class="sd">	================</span>
<span class="sd">		Returns the probability p(z=k|x, params)</span>
<span class="sd">		Input:</span>
<span class="sd">			X (N,D)	data points</span>
<span class="sd">		Output:</span>
<span class="sd">			p_gating (N,K)	probabilities of each gating function</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">pi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gating</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1">#p(z_i = k|x_i) (N,K)</span>
		<span class="c1">#pi = np.divide(pi.T, np.sum(pi, axis = 1)).T</span>
		<span class="k">return</span> <span class="n">pi</span></div>

<div class="viewcode-block" id="MoE_model.predict"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.MoE_model.predict">[docs]</a>	<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	predict</span>
<span class="sd">	=======</span>
<span class="sd">		Return the predictions of the model</span>
<span class="sd">		Input:</span>
<span class="sd">			X (N,D)	test points</span>
<span class="sd">		Output:</span>
<span class="sd">			y (N,)	model value at test points</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span><span class="mi">1</span><span class="p">:</span>
			<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
		
		<span class="n">pi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gating</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1">#p(z_i = k|x_i) (N,K)</span>

		<span class="c1">#indices = np.argmax(pi, axis =1)</span>
		<span class="c1">#for i in range(pi.shape[0]):</span>
		<span class="c1">#	pi[i,indices[i]] = 1.</span>

		<span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">pi</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
		<span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">experts_predictions</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
		<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span></div>

<div class="viewcode-block" id="MoE_model.expert_likelihood"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.MoE_model.expert_likelihood">[docs]</a>	<span class="k">def</span> <span class="nf">expert_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span> <span class="c1">#give to it a proper name!!!</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	expert_likelihood</span>
<span class="sd">	=================</span>
<span class="sd">		Computes the quantity p(y_i|x_i, z_i=k) = N(y_i| &lt;w_k,x_i&gt;). This corresponds to the likelihood of each expert for having generated the data.</span>
<span class="sd">		Input:</span>
<span class="sd">			X (N,D)	data</span>
<span class="sd">			y (N,)	targets</span>
<span class="sd">		Output:</span>
<span class="sd">			pi_k (N,K)	N(y_i| &lt;w_k,x_i&gt;)</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">gaussians_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">experts_predictions</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1">#(N,K) X*W + b</span>
		<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span><span class="mi">1</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#(N,K)</span>

		<span class="c1">#print(&#39;sigma: &#39;, self.sigma)</span>
		<span class="n">res</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">gaussians_mean</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span> <span class="p">)</span> <span class="c1">#(N,K)</span>
		<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span> <span class="c1">#normalizing result</span></div>

<div class="viewcode-block" id="MoE_model.log_likelihood"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.MoE_model.log_likelihood">[docs]</a>	<span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	log_likelihood</span>
<span class="sd">	==============</span>
<span class="sd">		Computes the log_likelihood for the data given with formula: (Are you sure of the formula??)</span>
<span class="sd">			LL 	= sum_{i=1}^N log sum_{k=1}^K p(y_i|x_i, z_i=k) * p(z_i=k |x_i) =</span>
<span class="sd">				= sum_{i=1}^N log sum_{k=1}^K N(y_i | &lt;w_k,x_i&gt;) S(x_i)_k</span>
<span class="sd">		Input:</span>
<span class="sd">			X (N,D)	data</span>
<span class="sd">			y (N,)	targets for regression</span>
<span class="sd">		Output:</span>
<span class="sd">			LL	log-likelihood for the model</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
			<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))</span>
		<span class="n">exp_likelihood</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_likelihood</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
		<span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gating</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">exp_likelihood</span><span class="p">)</span> <span class="c1">#(N,K)</span>
		<span class="n">res</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">res</span><span class="p">)</span><span class="o">&lt;</span><span class="mf">1e-30</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">1e-30</span> <span class="c1">#small regularizer for LL</span>
		<span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">res</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span> <span class="c1">#(N,)</span>
		<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>

	<span class="k">def</span> <span class="nf">__initialise_smart</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	__initialise_smart</span>
<span class="sd">	==================</span>
<span class="sd">		Having seen the data makes a smart first guess (with farhtest point clustering) for responsibilities and fit gating function model with those responbility.</span>
<span class="sd">		Input:</span>
<span class="sd">			X (N,D)		train data</span>
<span class="sd">			args		arguments to be given to fit method of gating function</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">centroids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">))</span>
		<span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">:</span>
			<span class="n">data</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">10</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,:]</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">data</span> <span class="o">=</span> <span class="n">X</span>
		<span class="n">N</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

			<span class="c1">#choosing centroids</span>
			<span class="c1">#points are chosen from dataset with farhtest point clustering</span>
		<span class="n">ran_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
		<span class="n">centroids</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">ran_index</span><span class="p">]</span>

		<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">):</span>
			<span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">k</span><span class="p">))</span> <span class="c1">#(N,K)</span>
			<span class="k">for</span> <span class="n">k_prime</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
				<span class="n">distances</span><span class="p">[:,</span><span class="n">k_prime</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">data</span> <span class="o">-</span> <span class="n">centroids</span><span class="p">[</span><span class="n">k_prime</span><span class="p">,:]),</span> <span class="n">axis</span> <span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#(N,K&#39;)</span>
			<span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#(N,)</span>
			<span class="n">distances</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span> <span class="c1">#normalizing distances to make it a prob vector</span>
			<span class="n">next_cl_arg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">p</span> <span class="o">=</span> <span class="n">distances</span><span class="p">)</span> <span class="c1">#chosen argument for the next cluster center</span>
			<span class="n">centroids</span><span class="p">[</span><span class="n">k</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">next_cl_arg</span><span class="p">,:]</span>

		<span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span> <span class="c1">#(D,)</span>

			<span class="c1">#computing initial responsibilities</span>
		<span class="n">r_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">))</span>
		<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">):</span>
			<span class="n">r_0</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">centroids</span><span class="p">[</span><span class="n">k</span><span class="p">,:]),</span> <span class="n">var</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-5</span>
		<span class="n">r_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">r_0</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">r_0</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">gating</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">r_0</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>

		<span class="k">return</span> <span class="n">r_0</span>



	<span class="k">def</span> <span class="nf">__initialise</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	__initialise</span>
<span class="sd">	============</span>
<span class="sd">		Having seen the data makes a first guess for responsibilities and fit gating function model with those responbility.</span>
<span class="sd">		Input:</span>
<span class="sd">			X (N,D)		train data</span>
<span class="sd">			args		arguments to be given to fit method of gating function</span>
<span class="sd">		&quot;&quot;&quot;</span>
			<span class="c1">#getting centroids</span>
		<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
		<span class="n">centroids</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">indices</span><span class="p">,:]</span> <span class="c1">#(K,D) #K centroids are chosen</span>
			<span class="c1">#getting variances</span>
		<span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span> <span class="c1">#(D,)</span>

			<span class="c1">#computing initial responsibilities</span>
		<span class="n">r_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">))</span>
		<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">):</span>
			<span class="n">r_0</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">centroids</span><span class="p">[</span><span class="n">k</span><span class="p">,:]),</span> <span class="n">var</span><span class="p">)[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-10</span>
		<span class="n">r_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">r_0</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">r_0</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">gating</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">r_0</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>

		<span class="k">return</span> <span class="n">r_0</span>

<div class="viewcode-block" id="MoE_model.fit"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.MoE_model.fit">[docs]</a>	<span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">N_iter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span> <span class="p">[],</span> <span class="n">verbose</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">val_set</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">pick_best</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	fit</span>
<span class="sd">	===</span>
<span class="sd">		Fit the model using EM algorithm.</span>
<span class="sd">		Input:</span>
<span class="sd">			X_train (N,D)	train data</span>
<span class="sd">			y_train (N,)	train targets for regression</span>
<span class="sd">			N_iter			Maximum number of iteration (if None only threshold is applied)</span>
<span class="sd">			threshold		Minimum change in LL below which algorithm is terminated</span>
<span class="sd">			args			arguments to be given to fit method of gating function</span>
<span class="sd">			verbose 		whether to print values during fit</span>
<span class="sd">			val_set			tuple (X_val, y_val) with a validation set to test performances</span>
<span class="sd">			pick_best		if True the model with best validation mse is chosen as best model (doesn&#39;t apply if val_set is None)</span>
<span class="sd">		Output:</span>
<span class="sd">			history		list of value for the LL of the model at every epoch</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="n">X_train</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
			<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))</span>
		<span class="k">if</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">:</span>
			<span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Wrong shape for X_train matrix &quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;. Second dimension should have lenght &quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">))</span>
		<span class="k">if</span> <span class="n">y_train</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
			<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>

		<span class="k">if</span> <span class="n">threshold</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">threshold</span> <span class="o">=</span> <span class="mi">0</span>

			<span class="c1">#initialization</span>
		<span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span><span class="p">:</span>
			<span class="n">r_0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__initialise_smart</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">args</span><span class="p">)</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">EM_step</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">r_0</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

		<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="n">old_LL</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1e5</span><span class="p">,)</span>
		<span class="n">LL</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1e4</span><span class="p">,)</span>
		<span class="k">if</span> <span class="n">pick_best</span><span class="p">:</span> <span class="n">best_mse</span> <span class="o">=</span> <span class="mf">1e6</span> <span class="c1">#best mse so far (only if val_set is not None)</span>
		<span class="n">history</span><span class="o">=</span><span class="p">[]</span>
		<span class="k">while</span><span class="p">(</span><span class="n">LL</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">old_LL</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span> <span class="p">):</span> <span class="c1">#exit condition is decided by train LL (even if val_set is not None): don&#39;t know why...</span>
				<span class="c1">#do batch update!!!</span>
			<span class="n">old_LL</span> <span class="o">=</span> <span class="n">LL</span>
			<span class="n">gat_history</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">EM_step</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="n">args</span><span class="p">)</span>
			<span class="n">LL</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),)</span>
			<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val_set</span><span class="p">,</span><span class="nb">tuple</span><span class="p">)</span> <span class="p">:</span>
				<span class="n">LL</span> <span class="o">=</span> <span class="p">(</span><span class="n">LL</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_likelihood</span><span class="p">(</span><span class="n">val_set</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">val_set</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
			<span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LL</span><span class="p">)</span>
			<span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
			<span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
				<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;LL at iter &quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;= &quot;</span><span class="p">,</span><span class="n">LL</span><span class="p">)</span>
				<span class="k">try</span><span class="p">:</span>
					<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Gating loss: &quot;</span><span class="p">,</span> <span class="n">gat_history</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">gat_history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
				<span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
					<span class="k">pass</span>
				<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val_set</span><span class="p">,</span><span class="nb">tuple</span><span class="p">)</span> <span class="p">:</span>
					<span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">val_set</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">-</span><span class="n">val_set</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span><span class="o">/</span><span class="n">val_set</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
					<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Val loss: &quot;</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>
					<span class="k">if</span> <span class="n">pick_best</span><span class="p">:</span>
						<span class="k">if</span> <span class="n">mse</span> <span class="o">&lt;</span> <span class="n">best_mse</span><span class="p">:</span>
							<span class="n">best_mse</span> <span class="o">=</span> <span class="n">mse</span>
							<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Chosen the best!&quot;</span><span class="p">)</span>
							<span class="k">try</span><span class="p">:</span> <span class="c1">#saving best model so far</span>
								<span class="bp">self</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;temp_exp&quot;</span><span class="p">,</span> <span class="s2">&quot;temp_gat&quot;</span><span class="p">)</span> 
							<span class="k">except</span><span class="p">:</span>
								<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s2">&quot;rm -f temp_exp temp_gat&quot;</span><span class="p">)</span>
			<span class="k">if</span> <span class="n">N_iter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
				<span class="k">if</span> <span class="n">i</span><span class="o">&gt;=</span> <span class="n">N_iter</span><span class="p">:</span>
					<span class="k">break</span>
			<span class="k">try</span><span class="p">:</span>
				<span class="k">assert</span> <span class="n">LL</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">old_LL</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span><span class="mi">0</span> <span class="c1">#train LL must always increase in EM algorithm. Useful check</span>
			<span class="k">except</span><span class="p">:</span>
				<span class="k">break</span> <span class="c1">#if LL increase (and it shouldn&#39;t) EM should terminate</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">initialized</span> <span class="o">=</span>  <span class="kc">True</span>

		<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val_set</span><span class="p">,</span><span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="n">pick_best</span><span class="p">:</span> <span class="c1">#loading best model so far</span>
			<span class="n">files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
			<span class="k">if</span> <span class="s2">&quot;temp_exp&quot;</span> <span class="ow">in</span> <span class="n">files</span> <span class="ow">and</span>  <span class="s2">&quot;temp_gat&quot;</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
				<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loaded the best&quot;</span><span class="p">)</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">load</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;temp_exp&quot;</span><span class="p">,</span> <span class="s2">&quot;temp_gat&quot;</span><span class="p">)</span>
				<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s2">&quot;rm -f temp_exp temp_gat&quot;</span><span class="p">)</span>

		<span class="k">return</span> <span class="n">history</span></div>

<div class="viewcode-block" id="MoE_model.EM_step"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.MoE_model.EM_step">[docs]</a>	<span class="k">def</span> <span class="nf">EM_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="p">[]):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	EM_step</span>
<span class="sd">	=======</span>
<span class="sd">		Does one EM update.</span>
<span class="sd">		Input:</span>
<span class="sd">			X (N,D)	train data</span>
<span class="sd">			y (N,)	train targets for regression</span>
<span class="sd">			r (N,K)	responsibilities for the E step (if None they are computed with method get_responsibilities)</span>
<span class="sd">			args	some arguments to pass to fit method of gating function</span>
<span class="sd">		Output:</span>
<span class="sd">			gat_history		history for the gating function fit</span>
<span class="sd">		&quot;&quot;&quot;</span>
			<span class="c1">#E step</span>
		<span class="k">if</span> <span class="n">r</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">r</span><span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_responsibilities</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="c1">#(N,K)</span>

			<span class="c1">#M step</span>
			<span class="c1">#M step for experts</span>
				<span class="c1">#weights is updated by solving a linear fit with weights r_{ik} in loss function</span>
		<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
			<span class="n">X_temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span><span class="n">X</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">X_temp</span> <span class="o">=</span> <span class="n">X</span>
		<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">):</span>
			<span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">r</span><span class="p">[:,</span><span class="n">k</span><span class="p">])</span> <span class="c1">#(N,N)</span>
			<span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X_temp</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">X_temp</span><span class="p">)))</span> <span class="c1">#(D,D)/(D+1,D+1)</span>
			<span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">temp</span><span class="p">,</span> <span class="n">X_temp</span><span class="o">.</span><span class="n">T</span><span class="p">),</span><span class="n">R</span><span class="p">)</span> <span class="c1">#(D,N)/(D+1,N)</span>
			<span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">temp</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1">#(D,)/(D+1,)</span>
			<span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1">#()</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="c1">#(D,)</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span> <span class="c1">#(D,)</span>
			<span class="n">sigma_square</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">r</span><span class="p">[:,</span><span class="n">k</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">experts_predictions</span><span class="p">(</span><span class="n">X</span><span class="p">)[:,</span><span class="n">k</span><span class="p">]))</span> <span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">r</span><span class="p">[:,</span><span class="n">k</span><span class="p">])</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma_square</span><span class="p">)</span>

			<span class="c1">#M step for gating functions</span>
		<span class="n">gat_history</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gating</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">r</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">gat_history</span></div>

<div class="viewcode-block" id="MoE_model.get_responsibilities"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.MoE_model.get_responsibilities">[docs]</a>	<span class="k">def</span> <span class="nf">get_responsibilities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	get_responsibilities</span>
<span class="sd">	====================</span>
<span class="sd">		Computes responsibilities for the given input data:</span>
<span class="sd">			r_k = p(y=k|x)</span>
<span class="sd">		Input:</span>
<span class="sd">			X (N,D)	data</span>
<span class="sd">			y (N,)	data labels</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">pi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gating</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1">#p(z_i = k|x_i) (N,K)</span>
		<span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">pi</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
		<span class="n">exp_term</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expert_likelihood</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

		<span class="n">r</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span>  <span class="n">exp_term</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span> <span class="c1"># (N,K) responsibilities matrix</span>
		<span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
		<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">r</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">)</span> 								<span class="c1">#checking that all r_{ik} are more than zero</span>
		<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">&lt;</span><span class="mf">1e-5</span><span class="p">)</span> 	<span class="c1">#checking proper normalization</span>

		<span class="c1">#r_new = np.zeros(r.shape)</span>
		<span class="c1">#print(&quot;r&quot;,r)</span>
		<span class="c1">#indices = np.argmax(r, axis =1)</span>
		<span class="c1">#print(indices)</span>
		<span class="c1">#for i in range(r.shape[0]):</span>
		<span class="c1">#	r_new[i,indices[i]] = 1.</span>
		<span class="c1">#print(&quot;r_new&quot;, r_new)</span>
		<span class="k">return</span> <span class="n">r</span></div>

<div class="viewcode-block" id="MoE_model.get_gradient"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.MoE_model.get_gradient">[docs]</a>	<span class="k">def</span> <span class="nf">get_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	get_gradient</span>
<span class="sd">	============</span>
<span class="sd">		Returns the gradient of the prediction y:</span>
<span class="sd">			grad_ni = D_i y_n</span>
<span class="sd">		where grad has shape (D,) and D_i denotes the partial derivative w.r.t. x_i.</span>
<span class="sd">		Input:</span>
<span class="sd">			X (N,D)</span>
<span class="sd">		Output:</span>
<span class="sd">			grad (N,D)</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span>
		<span class="n">jac_S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gating</span><span class="o">.</span><span class="n">get_jacobian</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1">#(N,K,D)</span>
		<span class="n">S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gating</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1">#(N,K)</span>
			<span class="c1">#self.W (D,K)</span>
		<span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span><span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="c1">#(N,K)</span>
		<span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">pred</span><span class="p">[:,:,</span><span class="kc">None</span><span class="p">],</span><span class="n">jac_S</span><span class="p">)</span> <span class="c1">#(N,K,D)</span>
		<span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
		<span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="c1">#(N,D)</span>
		<span class="c1">#print(X.shape,S.shape,jac_S.shape, grad.shape) #DEBUG</span>
		<span class="k">return</span> <span class="n">grad</span></div></div>

<span class="c1">################# softmax_regression class</span>
<div class="viewcode-block" id="softmax_regression"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.softmax_regression">[docs]</a><span class="k">class</span> <span class="nc">softmax_regression</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
	<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">softmax_regression</span>
<span class="sd">==================</span>
<span class="sd">	Implements a class for softmax regression with K labels. It has the form:</span>
<span class="sd">		p(y= k|x,V) = exp( V_k*x ) / sum_{k=1}^K exp( V_k*x + b_k )</span>
<span class="sd">	It has methods for getting predictions from the model and to fit the model.</span>
<span class="sd">	&quot;&quot;&quot;</span>
	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	__init__</span>
<span class="sd">	========</span>
<span class="sd">		Initialize the model with K classes for regressions.</span>
<span class="sd">		Input:</span>
<span class="sd">			D 	dimensionality of input space</span>
<span class="sd">			K	number or classes for regression</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="n">D</span> 
		<span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">K</span> 
		<span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">D</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">K</span><span class="p">))</span>
		<span class="k">return</span>

<div class="viewcode-block" id="softmax_regression.save"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.softmax_regression.save">[docs]</a>	<span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	save</span>
<span class="sd">	====</span>
<span class="sd">		Save softmax model to file.</span>
<span class="sd">		Input:</span>
<span class="sd">			filename	name of the file to save the model to</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">)</span>
		<span class="k">return</span></div>

<div class="viewcode-block" id="softmax_regression.load"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.softmax_regression.load">[docs]</a>	<span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	load</span>
<span class="sd">	====</span>
<span class="sd">		Load the model from file.</span>
<span class="sd">		Input:</span>
<span class="sd">			filename	name of the file to load the model from</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
		<span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="softmax_regression.predict"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.softmax_regression.predict">[docs]</a>	<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	predict</span>
<span class="sd">	=======</span>
<span class="sd">		Makes predictions for the softmax regression model. Weights can be freely specified by the user.</span>
<span class="sd">		Input:</span>
<span class="sd">			X_test (N,D)/(N,)	test points</span>
<span class="sd">			V (D+1,K)			weight of the model that gives predictions (if None, internal weights are used)</span>
<span class="sd">		Output:</span>
<span class="sd">			y_test (N,K)	model prediction</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="n">X_test</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
			<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
		<span class="k">if</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">:</span>
			<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span> <span class="n">X_test</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#adding dummy variable for the bias</span>
		<span class="k">if</span> <span class="n">V</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span>
		<span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">V</span><span class="p">)</span> <span class="c1">#(N,K)</span>
		<span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-5</span> <span class="c1">#(N,K)</span>
		<span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span> <span class="c1">#(N,K) normalizing</span>

		<span class="k">return</span> <span class="n">res</span></div>

<div class="viewcode-block" id="softmax_regression.get_jacobian"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.softmax_regression.get_jacobian">[docs]</a>	<span class="k">def</span> <span class="nf">get_jacobian</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	get_jacobian</span>
<span class="sd">	============</span>
<span class="sd">		Returns the jacobian of the softmax function:</span>
<span class="sd">			J_ki = D_i S(V^T x)_k</span>
<span class="sd">		where J has shape (K,D) and D_i denotes the partial derivative w.r.t. x_i.</span>
<span class="sd">		Input:</span>
<span class="sd">			X (N,D)</span>
<span class="sd">		Output:</span>
<span class="sd">			grad (N,K,D)</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span>
		<span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="mi">1</span><span class="p">:,:]</span><span class="o">.</span><span class="n">T</span> <span class="c1">#(K,D)</span>
		<span class="n">softmax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1">#(N,K)</span>
		<span class="n">jac1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">softmax</span><span class="p">[:,:,</span><span class="kc">None</span><span class="p">],</span> <span class="n">V</span><span class="p">[</span><span class="kc">None</span><span class="p">,:,:])</span> <span class="c1">#(N,K,D)</span>
		<span class="n">jac2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">softmax</span><span class="p">,</span><span class="n">V</span><span class="p">)</span> <span class="c1">#(N,D)</span>
		<span class="n">jac2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">jac2</span><span class="p">[:,</span><span class="kc">None</span><span class="p">,:],</span> <span class="n">softmax</span><span class="p">[:,:,</span><span class="kc">None</span><span class="p">])</span> <span class="c1">#(N,K,D)</span>
		<span class="n">jac</span> <span class="o">=</span> <span class="n">jac1</span> <span class="o">-</span><span class="n">jac2</span> <span class="c1">#(N,K,D)</span>
		<span class="c1">#print(&quot;ciao&quot;,softmax.shape,jac1.shape,jac2.shape, jac.shape) #DEBUG</span>
		<span class="k">return</span> <span class="n">jac</span> <span class="c1">#(N,K,D)</span></div>
		

<div class="viewcode-block" id="softmax_regression.fit_single_loop"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.softmax_regression.fit_single_loop">[docs]</a>	<span class="k">def</span> <span class="nf">fit_single_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	fit_single_loop</span>
<span class="sd">	===============</span>
<span class="sd">		######## DOES NOT WORK YET!! WE ARE SORRY FOR THE INCONVENIENT ######</span>
<span class="sd">		Fit the model using the closed form of LL of the problem.</span>
<span class="sd">		See: https://link.springer.com/content/pdf/10.1007%2F978-3-642-01510-6_109.pdf </span>
<span class="sd">		Input:</span>
<span class="sd">			X_train (N,D)	train data</span>
<span class="sd">			y_train (N,)	train targets for regression</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">:</span>
			<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span> <span class="n">X_train</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#adding dummy variable for the bias</span>

		<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
			<span class="c1">#print(np.where(y_train[:,-1]+2e-5 ==0))</span>
			<span class="n">div</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">y_train</span><span class="p">[:,</span><span class="n">k</span><span class="p">],</span><span class="n">y_train</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mf">2e-10</span><span class="p">)</span>
			<span class="n">div</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">div</span> <span class="o">==</span><span class="mi">0</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">1e-20</span>
			<span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">div</span><span class="p">)</span> <span class="c1"># (N,) &quot;new targets for linear regression&quot;</span>
			<span class="n">fitted_V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">X_train</span><span class="p">)),</span> <span class="n">X_train</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="c1">#(D+1,N)</span>
			<span class="n">fitted_V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">fitted_V</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[:,</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">fitted_V</span>

		<span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="o">+</span><span class="mi">1</span><span class="p">,))</span> <span class="c1">#last is zero by default! Remember it!</span>

		<span class="n">non_zero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">)</span>
		<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&lt;0: &quot;</span><span class="p">,</span><span class="n">non_zero</span><span class="p">)</span>
		<span class="k">return</span> </div>

<div class="viewcode-block" id="softmax_regression.accuracy"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.softmax_regression.accuracy">[docs]</a>	<span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	accuracy</span>
<span class="sd">	========</span>
<span class="sd">		Computes the accuracy of the model (i.e. the fraction of misclassified points).</span>
<span class="sd">		This measure is meaningful only in the case of hard clustering where there is only one label for each data point.</span>
<span class="sd">		Input:</span>
<span class="sd">			X_test (N,D)	test points</span>
<span class="sd">			y_test (N,K)	true labels of test points</span>
<span class="sd">		Output:</span>
<span class="sd">			accuracy	accuracy of the predictions made at test points</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="n">X_test</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
			<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))</span>
		<span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span></div>


<div class="viewcode-block" id="softmax_regression.LL"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.softmax_regression.LL">[docs]</a>	<span class="k">def</span> <span class="nf">LL</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	LL</span>
<span class="sd">	==</span>
<span class="sd">		Evaluate the log-likelihood for the model given X and their labels.</span>
<span class="sd">		Input:</span>
<span class="sd">			X_test (N,D)	test points</span>
<span class="sd">			y_test (N,K)	labels of test points</span>
<span class="sd">		Output:</span>
<span class="sd">			LL	log-likelihood for the model</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="n">X_test</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
			<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">,</span> <span class="p">[</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span> <span class="o">*</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>
		

<div class="viewcode-block" id="softmax_regression.loss"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.softmax_regression.loss">[docs]</a>	<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	loss</span>
<span class="sd">	====</span>
<span class="sd">		Loss function to minimize wrt. V. It is the function:</span>
<span class="sd">			NLL(V) = - log[p(D|V)]</span>
<span class="sd">		Input:</span>
<span class="sd">			V ((D+1)*K,)	weights of logreg</span>
<span class="sd">			data 			list [X_train (N,D), y_train (N,K), lambda ()]</span>
<span class="sd">		Output:</span>
<span class="sd">			loss	value for the loss function evaluated at V	</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">))</span>
		<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
		<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
		<span class="n">reg_constant</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

		<span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span> <span class="c1">#(N,K)</span>
			<span class="c1">#mu must be regularized in logarithm. Otherwise it might give Nan if a label prob is 0</span>
		<span class="n">LL</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu</span><span class="o">+</span><span class="mf">1e-40</span><span class="p">)))</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span> <span class="n">reg_constant</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">V</span><span class="p">))</span>
		<span class="k">return</span> <span class="n">LL</span></div>

<div class="viewcode-block" id="softmax_regression.grad"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.softmax_regression.grad">[docs]</a>	<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	grad</span>
<span class="sd">	====</span>
<span class="sd">		Gradient of the loss function to minimize wrt. V (see function loss())</span>
<span class="sd">		Input:</span>
<span class="sd">			V ((D+1)*K,)	weights of logreg</span>
<span class="sd">			data 			list [X_train (N,D), y_train (N,K), lambda ()]</span>
<span class="sd">		Output:</span>
<span class="sd">			grad	value for the gradient of loss function evaluated at V	</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">to_reshape</span> <span class="o">=</span> <span class="kc">False</span>
		<span class="k">if</span> <span class="n">V</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
			<span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">))</span>
			<span class="n">to_reshape</span> <span class="o">=</span> <span class="kc">True</span>
		<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
		<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
		<span class="n">reg_constant</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="c1">#regularizer</span>
		
		<span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span> <span class="c1">#(N,K)</span>
		<span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-4</span>
		<span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">delta</span><span class="p">)</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">reg_constant</span><span class="o">*</span><span class="n">V</span> <span class="c1">#(N,D).T (N,K) = (D,K)</span>
		<span class="k">if</span> <span class="n">to_reshape</span><span class="p">:</span>
			<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,))</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="k">return</span> <span class="n">grad</span></div>
	
<div class="viewcode-block" id="softmax_regression.get_weights"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.softmax_regression.get_weights">[docs]</a>	<span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	get_weights</span>
<span class="sd">	===========</span>
<span class="sd">		Returns the weights of the model.</span>
<span class="sd">		Input:</span>
<span class="sd">		Output:</span>
<span class="sd">			V (D+1,K)	weights for the model</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span></div>

<div class="viewcode-block" id="softmax_regression.fit"><a class="viewcode-back" href="../../api_reference/EM_MoE.html#mlgw.EM_MoE.softmax_regression.fit">[docs]</a>	<span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="s2">&quot;adam&quot;</span><span class="p">,</span> <span class="n">val_set</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reg_constant</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="n">N_iter</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	fit</span>
<span class="sd">	===</span>
<span class="sd">		Fit the model using gradient descent.</span>
<span class="sd">		Can use adam for adaptive step: https://arxiv.org/abs/1412.6980v8</span>
<span class="sd">		Can use bfgs method provided by scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_bfgs.html</span>
<span class="sd">		Using a small regularizer can help convergence (especially for bfgs).</span>
<span class="sd">		Input:</span>
<span class="sd">			X_train (N,D)	train data</span>
<span class="sd">			y_train (N,)	train targets for regression</span>
<span class="sd">			opt				which optimizer to use (&quot;adam&quot; or &quot;bsfg&quot;)</span>
<span class="sd">			val_set			tuple (X_val, y_val) with a validation set to test performances</span>
<span class="sd">			reg_constant	regularization constants</span>
<span class="sd">			verbose			whether to print values of loss function at every train step</span>
<span class="sd">			threshold		minimun improvement of validation erorr on 10 iteration before stopping (train error if val_set =None)</span>
<span class="sd">			N_iter			number of iteration to be performed (doesn&#39;t apply to bfgs or if threshold is not None)</span>
<span class="sd">			learning_rate	learning rate used for gradient update (doesn&#39;t apply to bfgs)</span>
<span class="sd">		Output:</span>
<span class="sd">			history		list of value for the loss function</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="k">if</span> <span class="n">X_train</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
			<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))</span>
		<span class="k">if</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="p">:</span>
			<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span> <span class="n">X_train</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#not necessary but might be useful to speed up the code</span>

		<span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">reg_constant</span><span class="p">)</span> <span class="c1">#arguments for loss and gradients</span>
		
		<span class="k">if</span> <span class="n">opt</span> <span class="o">==</span> <span class="s2">&quot;adam&quot;</span><span class="p">:</span>
			<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__optimize_adam</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">N_iter</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="n">val_set</span><span class="p">)</span>
		<span class="k">if</span> <span class="n">opt</span> <span class="o">==</span> <span class="s2">&quot;bfgs&quot;</span><span class="p">:</span>
			<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__optimise_bfgs</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="n">val_set</span><span class="p">)</span></div>

	<span class="k">def</span> <span class="nf">__optimise_bfgs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="n">val_set</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	__optimise_bfgs</span>
<span class="sd">	===============</span>
<span class="sd">		Wrapper to scipy.optimize.fmin_bfgs (https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_bfgs.html) to minimise loss function.</span>
<span class="sd">		Input:</span>
<span class="sd">			args		tuple of arguments to be passed to loss and gradient [X_train (N,D), y_train (N,K), lambda ()]</span>
<span class="sd">			verbose		whether to print scipy convergence message</span>
<span class="sd">			val_set		tuple (X_val, y_val) with a validation set to test performances</span>
<span class="sd">		Output:</span>
<span class="sd">			history		initial and final value of loss function</span>
<span class="sd">		&quot;&quot;&quot;</span>
		<span class="n">loss_0</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">,</span> <span class="n">args</span><span class="p">),)</span>
		<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val_set</span><span class="p">,</span><span class="nb">tuple</span><span class="p">):</span>
			<span class="n">args_val</span> <span class="o">=</span> <span class="p">(</span><span class="n">val_set</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">val_set</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
			<span class="n">loss_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">,</span> <span class="n">args_val</span><span class="p">)</span>
			<span class="n">loss_0</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_0</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">)</span>

			<span class="c1">#wrapper to self.loss and self.grad to make them suitable for scipy</span>
		<span class="n">loss</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">V</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">V</span><span class="p">,(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">))</span>
		<span class="n">grad</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">V</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">V</span><span class="p">,(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">))</span>

		<span class="n">res</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">fmin_bfgs</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">.</span><span class="n">reshape</span><span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,)),</span> <span class="n">grad</span><span class="p">,</span> <span class="n">args</span> <span class="p">,</span> <span class="n">disp</span> <span class="o">=</span> <span class="n">verbose</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">))</span>

		<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val_set</span><span class="p">,</span><span class="nb">tuple</span><span class="p">):</span>
			<span class="n">loss_fin</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">,</span> <span class="n">args</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">,</span> <span class="n">args_val</span><span class="p">))</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">loss_fin</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">,</span> <span class="n">args</span><span class="p">),</span> <span class="p">)</span>
		<span class="k">return</span> <span class="p">[</span><span class="n">loss_0</span><span class="p">,</span> <span class="n">loss_fin</span><span class="p">]</span>


	<span class="k">def</span> <span class="nf">__optimize_adam</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">N_iter</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">verbose</span><span class="p">,</span> <span class="n">val_set</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
		<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	__optimise_adam</span>
<span class="sd">	===============</span>
<span class="sd">		Implements optimizer with to perform adaptive step gradient descent.</span>
<span class="sd">		The implementation follows: https://arxiv.org/abs/1412.6980v8</span>
<span class="sd">		Input:</span>
<span class="sd">			args			tuple of arguments to be passed to loss and gradient [X_train (N,D), y_train (N,K), lambda ()]</span>
<span class="sd">			threshold		minimun improvement of train erorr before stopping fitting procedure</span>
<span class="sd">			N_iter			number of iteration to be performed</span>
<span class="sd">			learning_rate	learning rate to be set for adam</span>
<span class="sd">			verbose			whether to print loss at each step</span>
<span class="sd">			val_set			tuple (X_val, y_val) with a validation set to test performances</span>
<span class="sd">		Output:</span>
<span class="sd">			history		list of loss function value (train, )/(train,val) at each iteration step</span>
<span class="sd">		&quot;&quot;&quot;</span>
			<span class="c1">#setting parameters for learning rate</span>
		<span class="n">beta1</span> <span class="o">=</span> <span class="mf">.9</span>		<span class="c1">#forgetting factor for first moment</span>
		<span class="n">beta2</span> <span class="o">=</span> <span class="mf">.999</span>	<span class="c1">#forgetting factor for second moment</span>
		<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span>
		<span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1">#first moment</span>
		<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1">#second moment</span>
		<span class="n">history</span> <span class="o">=</span> <span class="p">[]</span>
		<span class="k">if</span> <span class="n">threshold</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
			<span class="n">N_iter</span> <span class="o">=</span> <span class="mi">1000000000</span> <span class="c1"># if threshold, no maximum iteration should be used</span>
		<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">N_iter</span><span class="p">):</span>
			<span class="n">g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
			<span class="n">m</span> <span class="o">=</span> <span class="n">beta1</span><span class="o">*</span><span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta1</span><span class="p">)</span><span class="o">*</span><span class="n">g</span>
			<span class="n">v</span> <span class="o">=</span> <span class="n">beta2</span><span class="o">*</span><span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
			<span class="n">m_corr</span> <span class="o">=</span> <span class="n">m</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta1</span><span class="p">)</span>
			<span class="n">v_corr</span> <span class="o">=</span> <span class="n">v</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta2</span><span class="p">)</span>

			<span class="n">update</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">m_corr</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v_corr</span><span class="p">)</span><span class="o">+</span><span class="n">epsilon</span><span class="p">)</span>
			<span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">update</span><span class="p">)):</span> <span class="c1">#debug</span>
				<span class="n">quit</span><span class="p">()</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">update</span>
			<span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="o">+</span><span class="mi">1</span><span class="p">,))</span>

			<span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val_set</span><span class="p">,</span><span class="nb">tuple</span><span class="p">):</span>
				<span class="n">args_val</span> <span class="o">=</span> <span class="p">(</span><span class="n">val_set</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">val_set</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
				<span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">,</span> <span class="n">args</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">,</span> <span class="n">args_val</span><span class="p">))</span> <span class="p">)</span> <span class="c1">#(train_err, val_err)</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">,</span> <span class="n">args</span><span class="p">),))</span>

			<span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
				<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loss at iter= &quot;</span><span class="p">,</span><span class="n">i</span><span class="p">,</span> <span class="n">history</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

			<span class="k">if</span> <span class="n">threshold</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">10</span><span class="p">:</span>
				<span class="k">if</span> <span class="n">history</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">history</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
					<span class="k">break</span>
		<span class="k">return</span> <span class="n">history</span></div>




</pre></div>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Stefano Schmidt
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
       Copyright 2023, Stefano Schmidt.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>